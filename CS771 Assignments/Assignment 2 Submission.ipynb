{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS771 Assignment 2 Submission for Adarsh Pal (180032)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "J1eJE5y06MNS"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.linalg as lin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sTSJ8iWK6PwC"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(gradient,init_,learn_rate, n_iter=50, tol=1e-06):\n",
    "    x = init_\n",
    "    for _ in range(n_iter):\n",
    "        delta = -learn_rate*gradient(x)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return round(x*1000)/1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9xJx80zKl7jz"
   },
   "source": [
    "# Question 1(a)\n",
    "Clearly, for $f(x)=x^2+3x+4$, we get $f'(x)=2x+3$ and $f''(x)=2 > 0$. Hence, we see that $f(x)$ is convex and so, we will be converging to its global minima, no matter what initial values we take, as long as the learn_rate is small enough that we do not overshoot.<br>\n",
    "For $f(x)=x^4-3x^2+2x$, we get $f'(x)=4x^3-6x+2$ and $f''(x)=12x^2-6$. Clearly, this is not a convex function as $f''(x)$ is not positive for every $x$ and so, optimal minima will depend on the initial values chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDeyMf1c6SH_",
    "outputId": "4b343e7d-fa26-4708-ac64-47aa1a2995e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local Minima for x^2+3x+4= -1.5\n",
      "Local Minima for x^4-3x^2+2x with init=-5:  -1.366\n",
      "Local Minima for x^4-3x^2+2x with init=5:  1.0\n"
     ]
    }
   ],
   "source": [
    "# The local minima is also the global minima since the function is convex\n",
    "print(\"Local Minima for x^2+3x+4=\", gradient_descent(lambda x:2*x+3,init_=-10,learn_rate=0.01,n_iter=10000))  \n",
    "# The local minima is not the global minima as function is not convex\n",
    "print(\"Local Minima for x^4-3x^2+2x with init=-5: \", gradient_descent(lambda x:4*(x**3)-6*x+2,init_=-5,learn_rate=0.001,n_iter=10000))\n",
    "print(\"Local Minima for x^4-3x^2+2x with init=5: \", gradient_descent(lambda x:4*(x**3)-6*x+2,init_=5,learn_rate=0.001,n_iter=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihdnXlOiooKm"
   },
   "source": [
    "# Question 1(b)\n",
    "Let us define <br>\n",
    "$w$ =\n",
    "\\begin{bmatrix}\n",
    "b \\\\\n",
    "a\n",
    "\\end{bmatrix}<br>\n",
    "$X$ =\n",
    "\\begin{bmatrix}\n",
    "1& x_{1} \\\\\n",
    "1& x_{2} \\\\\n",
    "\\vdots \\\\\n",
    "1& x_{n}\n",
    "\\end{bmatrix}<br>\n",
    "$y$ =\n",
    "\\begin{bmatrix}\n",
    " y_{1} \\\\\n",
    " y_{2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{n}\n",
    "\\end{bmatrix}, where n= number of training examples<br>\n",
    "Then we need to minimize $f(w)=\\frac{1}{n} \\sum_{i=1}^n (y_i-X_i w)^2$, which can be written as<br>\n",
    "$f(w)=\\frac{1}{n} ||y-Xw||_2^2=\\frac{1}{n} (y-Xw)^T (y-Xw)=\\frac{1}{n} (y^T-w^TX^T)(y-Xw)\\\\ \\Rightarrow f(w)=\\frac{1}{n}(y^Ty-y^TXw-w^TX^Ty+w^TX^TXw)$<br>\n",
    "Hence, $\\nabla f(w)=\\frac{1}{n}(-(y^TX)^T-X^Ty+2X^TXw)=\\frac{1}{n}(-2X^Ty+2X^TXw)$<br>\n",
    "Hence, one step of gradient descent becomes:<br>\n",
    "$w^{(t+1)}=w^{(t)}-\\frac{\\alpha}{n} \\nabla f(w^{(t)})$ where $\\alpha=$ learning rate.<br>\n",
    "$\\Rightarrow w^{(t+1)}=w^{(t)}-\\frac{\\alpha}{n} (-2X^Ty+2X^TXw)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PLypj28lpAQn"
   },
   "outputs": [],
   "source": [
    "# This is the gradient function for linear regression\n",
    "#w is the column vector [b,a]\n",
    "def linear_regression_gradient(y,X,w):\n",
    "    n=y.shape[0]\n",
    "    y=np.reshape(y,(n,1))\n",
    "    #Adding a column of 1s to X\n",
    "    X=np.hstack((np.ones((n,1)),X.reshape(n,1)))\n",
    "    #Finding the gradient according to the formula above\n",
    "    gradient=(1/n)*(-2*np.matmul(np.transpose(X),y)+2*np.matmul(np.matmul(np.transpose(X),X),w))\n",
    "    return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JpK3mvrg6V66"
   },
   "outputs": [],
   "source": [
    "#w is the column vector [b,a]\n",
    "def linear_regression(gradient,y,X,init_a,init_b,learn_rate, n_iter=50, tol=1e-06):\n",
    "    #Initialize w to [init_b,init_a]\n",
    "    w=np.array([init_b,init_a])\n",
    "    w=np.reshape(w,(2,1))\n",
    "    for _ in range(n_iter):\n",
    "        delta=-learn_rate*gradient(y,X,w)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        w += delta\n",
    "    w=np.squeeze(w)\n",
    "    return (w[1],w[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tys0m2m3AKdp"
   },
   "source": [
    "# Question 1 (c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "IP4fmK3w6WwT"
   },
   "outputs": [],
   "source": [
    "#Generation of data\n",
    "np.random.seed(0)\n",
    "X=2.5*np.random.randn(10000)+1.5\n",
    "res=1.5*np.random.randn(10000)\n",
    "y=2+0.3*X+res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AbeMkWzB6X_l",
    "outputId": "b36f9717-296f-482b-f49a-86d8a479b6e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= 0.2953197350589738 b= 2.0232822054084147\n"
     ]
    }
   ],
   "source": [
    "# Finding optimal values of a and b using gradient descent\n",
    "(a,b)=linear_regression(linear_regression_gradient,y,X,init_a=1.,init_b=1.,learn_rate=0.1,n_iter=10000,tol=1e-06)\n",
    "print(\"a=\",a,\"b=\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I_ynpIB41BZh"
   },
   "source": [
    "# Question 1 (d)\n",
    "In Minibatch Gradient Descent, we will just be randomly taking k training examples $(x_i$ $_j$,$y_i$ $_j$), $j=1,2,...,k$ and apply gradient descent on them. Hence,<br>\n",
    "$w$ =\n",
    "\\begin{bmatrix}\n",
    "b \\\\\n",
    "a\n",
    "\\end{bmatrix}<br>\n",
    "$X_{batch}$ = \\begin{bmatrix}\n",
    "1& x_{i_1} \\\\\n",
    "1& x_{i_2} \\\\\n",
    "\\vdots \\\\\n",
    "1& x_{i_k}\n",
    "\\end{bmatrix}<br>\n",
    "$y_{batch}$ = \\begin{bmatrix}\n",
    "y_{i_1} \\\\\n",
    "y_{i_2} \\\\\n",
    "\\vdots \\\\\n",
    "y_{i_k}\n",
    "\\end{bmatrix}<br>\n",
    "and the gradient descent step becomes $w^{(t+1)}=w^{(t)}-\\frac{\\alpha}{k} (-2X_{batch}^Ty_{batch}+2X_{batch}^TX_{batch}w)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "j1LvNIMJ6Yui"
   },
   "outputs": [],
   "source": [
    "#Minibatch Gradient Descent Implementation\n",
    "# Here k refers to the batch size\n",
    "def minibatch_gradient_descent(gradient,y,X,k,init_a,init_b,learn_rate, n_iter=50, tol=1e-06):\n",
    "    w=np.array([init_b,init_a])\n",
    "    w=np.reshape(w,(2,1))\n",
    "    for _ in range(n_iter):\n",
    "        # Choosing k random training examples\n",
    "        choose= np.random.choice(X.shape[0], k, replace=False)\n",
    "        Xsgd=X[choose]\n",
    "        ysgd=y[choose]\n",
    "        delta=-learn_rate*gradient(ysgd,Xsgd,w)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        w+=delta\n",
    "    w=np.squeeze(w)\n",
    "    return (w[1],w[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ziWZEFVgyl4X",
    "outputId": "f6f3d989-da06-4901-8cda-1c088a0641b2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= 0.28581950363309866 b= 2.067062816995244\n"
     ]
    }
   ],
   "source": [
    "# Taking k=1000\n",
    "(a,b)=minibatch_gradient_descent(linear_regression_gradient,y,X,k=1000,init_a=1.,init_b=1.,learn_rate=0.1,n_iter=10000,tol=1e-06)\n",
    "print(\"a=\",a,\"b=\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qkWmkDvqcq9k"
   },
   "source": [
    "# Question 1 (e) Comparing the performance of Minibatch Gradient Descent vs Normal Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2cnvTzoXrQGI",
    "outputId": "f3e6aeea-91de-42b6-e6fc-8fed76157285"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 loop, best of 5: 2.65 s per loop\n",
      "2 1 loop, best of 5: 2.66 s per loop\n",
      "4 1 loop, best of 5: 2.66 s per loop\n",
      "8 1 loop, best of 5: 2.68 s per loop\n",
      "16 1 loop, best of 5: 2.65 s per loop\n",
      "32 1 loop, best of 5: 2.69 s per loop\n",
      "64 1 loop, best of 5: 2.67 s per loop\n",
      "128 1 loop, best of 5: 2.72 s per loop\n",
      "256 1 loop, best of 5: 2.71 s per loop\n",
      "512 1 loop, best of 5: 2.76 s per loop\n",
      "1024 1 loop, best of 5: 2.81 s per loop\n",
      "2048 1 loop, best of 5: 2.93 s per loop\n",
      "4096 1 loop, best of 5: 3.21 s per loop\n",
      "8192 1 loop, best of 5: 5.82 s per loop\n"
     ]
    }
   ],
   "source": [
    "#Choosing batches of size 1,2,4,8,16,32,64,128,256,512,1024,2048,4096,8192\n",
    "batches= 2**np.arange(0,14)\n",
    "for i in batches:\n",
    "  print(i,end=\" \")\n",
    "  # Let's look at the times taken for learn_rate=0.1 and tol=1e-06\n",
    "  %timeit minibatch_gradient_descent(linear_regression_gradient,y,X,k=i,init_a=1.,init_b=1.,learn_rate=0.1,n_iter=10000,tol=1e-06)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_e_qficVrdX9",
    "outputId": "a7ea6083-6c90-4d83-d003-6133c6cf0f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 12.2 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the time taken by gradient descent for learn_rate=0.1 and tol=1e-06\n",
    "%timeit linear_regression(linear_regression_gradient,y,X,init_a=1.,init_b=1.,learn_rate=0.1,n_iter=10000,tol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpDQ4Ih_FAHo"
   },
   "source": [
    "Thus, we can see that for learn_rate=0.1 and tol=1e-06, Linear Regression runs faster than Minibatch Gradient Descent for all batch sizes. This is mainly because Minibatch Gradient Descent although takes lesser time for a single iteration of the for loop as compared to Gradient Descent, it takes a longer time to converge owing to lower tolerance and higher step size. We are not moving in the direction of steepest descent (since we are taking a subset of the total training examples only). As a result, minibatch gradient descent takes more ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T6-bXq2zCYKl",
    "outputId": "82ea4e77-7181-439a-f4a5-def25a185c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 10 loops, best of 5: 12.2 ms per loop\n",
      "2 The slowest run took 16.49 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 5: 16.8 ms per loop\n",
      "4 The slowest run took 25.02 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "1 loop, best of 5: 11.3 ms per loop\n",
      "8 1 loop, best of 5: 126 ms per loop\n",
      "16 10 loops, best of 5: 130 ms per loop\n",
      "32 1 loop, best of 5: 165 ms per loop\n",
      "64 1 loop, best of 5: 138 ms per loop\n",
      "128 1 loop, best of 5: 230 ms per loop\n",
      "256 1 loop, best of 5: 268 ms per loop\n",
      "512 1 loop, best of 5: 283 ms per loop\n",
      "1024 1 loop, best of 5: 357 ms per loop\n",
      "2048 1 loop, best of 5: 421 ms per loop\n",
      "4096 1 loop, best of 5: 503 ms per loop\n",
      "8192 1 loop, best of 5: 1.02 s per loop\n"
     ]
    }
   ],
   "source": [
    "#Choosing batches of size 1,2,4,8,16,32,64,128,256,512,1024,2048,4096,8192\n",
    "batches= 2**np.arange(0,14)\n",
    "for i in batches:\n",
    "  print(i,end=\" \")\n",
    "  # Let's look at the times taken for learn_rate=1e-3 and tol=1e-04\n",
    "  %timeit minibatch_gradient_descent(linear_regression_gradient,y,X,k=i,init_a=1.,init_b=1.,learn_rate=1e-3,n_iter=10000,tol=1e-04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KT75yihrCldB",
    "outputId": "12bbe4cf-fdbb-4983-ebf6-142af204e1da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 5: 300 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%timeit linear_regression(linear_regression_gradient,y,X,init_a=1.,init_b=1.,learn_rate=1e-3,n_iter=10000,tol=1e-04)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ges0x-r5F4eZ"
   },
   "source": [
    "Thus, we see that as we decrease the step size, the minibatch gradient descent gives a better performance than normal gradient descent for smaller values. This may be attributed to the smaller step size and the higher tolerance which prevents minibatch gradient descent from overshooting and oscillating around the minima. Also, we observe that the time taken is increasing with increasing batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vZkQyhpnFiqj"
   },
   "source": [
    "If we look at the error terms $|a_{predicted}-0.3|+|b_{predicted}-2.0|$ for the various batch sizes, they are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "G9bVE3nhSpf8"
   },
   "outputs": [],
   "source": [
    "def batch_size_losses():\n",
    "  batches=2**np.arange(0,14)\n",
    "  ab_loss=list()\n",
    "  for i in batches:\n",
    "    (a,b)=minibatch_gradient_descent(linear_regression_gradient,y,X,k=i,init_a=1.,init_b=1.,learn_rate=0.1,n_iter=10000,tol=1e-06)\n",
    "    ab_loss.append(np.abs(a-0.3)+np.abs(b-2))\n",
    "    print(\"batch_size=\",i,\"absolute_loss=\",ab_loss[-1],\"a=\",a,\"b=\",b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kLsk8V0yTbon",
    "outputId": "be2d85ac-569c-462f-fdaa-11e9a2098faf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size= 1 absolute_loss= 11.042689343976118 a= 0.06477362657146202 b= -8.80746297054758\n",
      "batch_size= 2 absolute_loss= 7.72096669697454 a= 3.3152971971996887 b= -2.705669499774851\n",
      "batch_size= 4 absolute_loss= 0.6053647561562137 a= -0.05313525281895548 b= 1.7477704966627419\n",
      "batch_size= 8 absolute_loss= 0.19812140478571422 a= 0.1587206105738348 b= 1.943157984640451\n",
      "batch_size= 16 absolute_loss= 1.2240908515725932 a= -0.6592777340278722 b= 1.735186882455279\n",
      "batch_size= 32 absolute_loss= 0.1880761826774575 a= 0.16962743233541652 b= 2.057703615012874\n",
      "batch_size= 64 absolute_loss= 0.28584315404288363 a= 0.40767951150703013 b= 2.1781636425358535\n",
      "batch_size= 128 absolute_loss= 0.20254988190233889 a= 0.11577891286204464 b= 2.0183287947643835\n",
      "batch_size= 256 absolute_loss= 0.0752590002319305 a= 0.37121228079374213 b= 2.0040467194381884\n",
      "batch_size= 512 absolute_loss= 0.03785932329357994 a= 0.2785325585466836 b= 2.0163918818402635\n",
      "batch_size= 1024 absolute_loss= 0.07275445456638738 a= 0.24664074265848487 b= 2.0193951972248723\n",
      "batch_size= 2048 absolute_loss= 0.022260735032840484 a= 0.2830128563620409 b= 2.0052735913948814\n",
      "batch_size= 4096 absolute_loss= 0.03030325930918254 a= 0.28552942842453016 b= 2.0158326877337127\n",
      "batch_size= 8192 absolute_loss= 0.02771304406887848 a= 0.30228283226191444 b= 2.025430211806964\n"
     ]
    }
   ],
   "source": [
    "batch_size_losses()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MRgkqoc-HrlC"
   },
   "source": [
    "Clearly, if we look at the losses for different batch sizes, we find out that any batch size >= 128 is fine, but as the batch size increases, more and more time is taken. Also, note that for batch size=1, the larger step size coupled with higher gradient is causing it to move towards infinity. Let us look at the validation error for different batch sizes as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ftnrQMvIcCBn"
   },
   "outputs": [],
   "source": [
    "# Splitting into training and validation data\n",
    "msk = np.random.rand(X.shape[0]) < 0.8\n",
    "X_train = X[msk]\n",
    "X_validation = X[~msk]\n",
    "y_train = y[msk]\n",
    "y_validation = y[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "WmlEqJwBdZLi"
   },
   "outputs": [],
   "source": [
    "def batch_size_RMSE(X_train,X_validation,y_train,y_validation):\n",
    "  batches=2**np.arange(0,13)\n",
    "  sq_loss=list()\n",
    "  for i in batches:\n",
    "    (a,b)=minibatch_gradient_descent(linear_regression_gradient,y_train,X_train,k=i,init_a=1.,init_b=1.,learn_rate=0.1,n_iter=10000,tol=1e-06)\n",
    "    rmse=1/X_validation.shape[0]*lin.norm(y_validation-a*X_validation-b)\n",
    "    print(\"batch_size=\",i,\"RMSE=\",rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K3PPHk5xegxt",
    "outputId": "11b5d06d-42da-4dbc-feb1-1c60e14cc241"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_size= 1 RMSE= 7.495120736093498e+50\n",
      "batch_size= 2 RMSE= 0.056767840102433305\n",
      "batch_size= 4 RMSE= 0.03556595947386352\n",
      "batch_size= 8 RMSE= 0.03803357839997114\n",
      "batch_size= 16 RMSE= 0.03419313971680664\n",
      "batch_size= 32 RMSE= 0.03461965916434229\n",
      "batch_size= 64 RMSE= 0.03361741225358691\n",
      "batch_size= 128 RMSE= 0.03505726287031953\n",
      "batch_size= 256 RMSE= 0.03346582284103738\n",
      "batch_size= 512 RMSE= 0.0334902608076817\n",
      "batch_size= 1024 RMSE= 0.03348280478592348\n",
      "batch_size= 2048 RMSE= 0.03343964548475806\n",
      "batch_size= 4096 RMSE= 0.03344639906604042\n"
     ]
    }
   ],
   "source": [
    "batch_size_RMSE(X_train,X_validation,y_train,y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qac76IoXglvV"
   },
   "source": [
    "Even using a validation set and finding the RMSE does not differentiate between batch sizes by much. Thus, we might end up getting minimal improvements w.r.t. accuracy, but the process will take more time with increasing batch size. Hence, taking time taken, squared_loss and RMSE into account, we see that a batch_size of 64 to 128 could be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6W89tTyqJRQf",
    "outputId": "5d499053-7740-4037-8d9a-9b2e3227076f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a= 0.18952200202830877 b= 2.044251411849159\n"
     ]
    }
   ],
   "source": [
    "(a,b)=minibatch_gradient_descent(linear_regression_gradient,y,X,k=128,init_a=1.,init_b=1.,learn_rate=0.1,n_iter=10000,tol=1e-06)\n",
    "print(\"a=\",a,\"b=\",b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i93qbYj20fLO"
   },
   "source": [
    "# Question 2: Bayesian Analysis\n",
    "Let us define a few terms: <br>\n",
    "$P(cold)$:= Probability that someone has a cold <br>\n",
    "$P(fever)$:= Probability that someone has a fever <br>\n",
    "$P(cough)$:= Probability that someone has a cough <br>\n",
    "$P(lung)$:= Probability that someone has a lung disease <br>\n",
    "$P(smokes)$:= Probability that someone smokes <br>\n",
    "(a) The question wants us to find $P(cold \\cap fever)$ <br> Clearly, $P(cold \\cap fever)=P(fever|cold)P(cold)=0.307*0.02=0.00614$ <br>\n",
    "(b) The question wants us to find $P(cold | cough)$. <br>\n",
    "Note that<br>\n",
    "$P(lung)=P(lung|smokes)*P(smokes)+P(lung|not\\ smokes)*P(not\\ smokes)$<br>\n",
    "$=0.1009*0.2+0.001*0.8=0.02098$, and <br>\n",
    "$P(lung|cold)=P(lung)=0.02098$ (As lung and cold are independent events) and <br>\n",
    "$P(not\\ lung|cold)=P(not\\ lung)=1-P(lung)=1-0.02098=0.97902$ (Same reason as above).<br>\n",
    "Now, $P(cough | cold)$<br>$=P(cough | cold \\cap lung)P(lung|cold)+P(cough | cold \\cap not\\ lung)P(not\\ lung|cold)$<br>$=P(cough | cold \\cap lung)P(lung)+P(cough | cold \\cap not\\ lung)P(not\\ lung)$<br>$=0.7525*0.02098+0.505*0.97902$<br>$=0.51019255$<br>\n",
    "Similarly,<br>\n",
    "$P(cough | not\\ cold)$<br>$=P(cough | not\\ cold \\cap lung)P(lung|not\\ cold)+P(cough | not\\ cold \\cap not\\ lung)P(not\\ lung|not\\ cold)$<br>$=P(cough | not\\ cold \\cap lung)P(lung)+P(cough | not\\ cold \\cap not\\ lung)P(not\\ lung)$<br>$=0.505*0.02098+0.01*0.97902=0.0203851$<br>\n",
    "Hence, by Bayes' Method,<br>\n",
    "$P(cold|cough)$<br>$=\\frac{P(cough|cold)P(cold)}{P(cough|cold)P(cold)+P(cough|not\\ cold)P(not\\ cold)}$<br>\n",
    "=$\\frac{0.51019255*0.02}{0.51019255*0.02+0.0203851*0.98}=\\frac{0.010203851}{0.010203851+0.019977398}=\\frac{0.010203851}{0.030181249}=0.338085776$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uIUObI_b90XZ"
   },
   "source": [
    "\n",
    "# Question 3: MLE for a Multinomial Distribution\n",
    "Let $X=(X_1,X_2,...,X_k)\\sim Mult(n,p_1,p_2,...,p_k)$ such that $X_i$ appears $x_i$ times. $\\sum_{j=1}^k x_j=n$, and $\\sum_{j=1}^k p_j =1$ Also, $x_i\\ge0$ and $p_i\\ge0$ for $j \\in \\{1,2,...,k\\}$.\n",
    "Then,<br>\n",
    "$L(X_1,X_2,...,X_k | (p_1,p_2,...,p_k))$<br>$\n",
    "=P(x_1,x_2,...,x_k)| (p_1,p_2,...,p_k))\\\\\n",
    "=\\frac{n!}{x_1!x_2! ... x_k!} p_1^{x_1} p_2^{x_2} ... p_k^{x_k}$, where $x_k=n-\\sum_{j=1}^{k-1} x_j$ and $p_k=1-\\sum_{j=1}^{k-1} p_j$<br>$\n",
    "\\Rightarrow log L(X_1,X_2,...,X_k| (p_1,p_2,...,p_k))$<br>$=\\log({ \\frac{n!}{x_1!x_2! ... x_k!} p_1^{x_1} p_2^{x_2} ... p_k^{x_k}})$<br>$\n",
    "=\\log{\\frac{n!}{x_1!x_2! ... x_k!}}+\\log{(p_1^{x_1})}+\\log{(p_2^{x_2})}+...+\\log{(p_k^{x_k})}$<br>$\n",
    "=\\log{\\frac{n!}{x_1!x_2! ... x_k!}} +x_1\\log{(p_1)}+x_2\\log{(p_2)}+...+x_k\\log{(p_k)}$<br>\n",
    "$\\therefore NLL(p_1,p_2,...,p_k):= -log L(X_1,X_2,...,X_k| (p_1,p_2,...,p_k))$<br>$=-\\log{\\frac{n!}{x_1!x_2! ... x_k!}} -x_1\\log{(p_1)}-x_2\\log{(p_2)}-...-x_k\\log{(p_k)}$<br>\n",
    "Hence, we need to find the values of $(p_1,p_2,...,p_k)$ minimizing $NLL(p_1,p_2,...,p_k)$\n",
    "This function does not have independent parameters as $\\sum_{j=1}^k p_j =1$<br>\n",
    "Hence, this is a constrained optimization problem with $\\sum_{j=1}^k p_j =1$.\n",
    "Hence, we will have to use a Lagrangian to find the optimal parameters for minimizing $NLL(p_1,p_2,...,p_k)$.<br>\n",
    "Take $F(p_1,p_2,...,p_k,\\lambda):=NLL(p_1,p_2,...,p_k)+\\lambda*(\\sum_{j=1}^k p_j-1)$<br>\n",
    "Hence, the optimization problem becomes the primal problem<br>\n",
    "$\\min_{(p_1,p_2,..,p_k)} (\\max_{\\lambda} (F(p_1,p_2,...,p_k,\\lambda)))$<br>\n",
    "Interchanging the min and the max (which we can do, since $p_is$ and $\\lambda$ have independent domains, we will get the dual problem\n",
    "$\\max_{\\lambda} (\\min_{(p_1,p_2,..,p_k)} (F(p_1,p_2,...,p_k,\\lambda)))$<br>\n",
    "For solving the above problem, we do the following:<br>\n",
    "For $j \\in \\{1,2,...,k\\}, \\frac{\\partial F}{\\partial p_j}=0 \\Rightarrow -\\frac{x_j}{p_j}+\\lambda=0 \\Rightarrow p_j=\\frac{x_j}{\\lambda}$<br>\n",
    "Also,<br>\n",
    "$\\sum_{j=1}^k p_j-1=0$<br>$\\Rightarrow \\lambda(\\sum_{j=1}^k p_j-1)=0 $<br>$\\Rightarrow (\\sum_{j=1}^k \\lambda p_j-\\lambda)=0$<br>$\\Rightarrow (\\sum_{j=1}^k x_j-\\lambda)=0$<br>$\\Rightarrow \\lambda=\\sum_{j=1}^k x_j=n$<br>$\n",
    " \\therefore (p_j)_{MLE}=\\frac{x_j}{\\lambda}=\\frac{x_j}{n}$<br>\n",
    "To show that $NLL(p_1,p_2,...,p_k)$ is semi-convex, we see that<br>\n",
    "for $i,j \\in \\{1,2,...,k\\}, \\frac{\\partial (NLL)}{\\partial p_j}= -\\frac{x_j}{p_j}+\\lambda \\Rightarrow \\frac{\\partial^2 (NLL)}{\\partial p_j^2}= \\frac{x_j}{p_j^2}\\ge 0 $ and<br>\n",
    "$\\frac{\\partial^2 (NLL)}{\\partial p_i \\partial p_j}=0$ for $i\\neq j$.<br>\n",
    "Hence the Hessian matrix becomes a diagonal matrix $D$ with diagonal entries $D_{jj}=\\frac{x_j}{p_j^2}\\ge 0$ Hence, the Hessian is a positive semi-definite matrix since all the eigenvalues of $D$ are non-negative and thus, $NLL(p_1,p_2,...,p_k)$ is semi-convex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment 2 Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
